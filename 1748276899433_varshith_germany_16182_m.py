# -*- coding: utf-8 -*-
"""VARSHITH GERMANY 16182 m

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhlE1nL9JJcQ85tduFSzTx_5CAFGK_P
"""

# Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings('ignore')

# Step 2: Load Dataset
df = pd.read_csv("/content/creditcard.csv")
print("Dataset Shape:", df.shape)

# Step 3: EDA
print(df.describe())
print(df['Class'].value_counts())

# Class distribution plot
sns.countplot(x='Class', data=df)
plt.title('Class Distribution')
plt.show()

# Step 4: Preprocessing
X = df.drop(['Class', 'Time'], axis=1)
y = df['Class']

# Normalize 'Amount'
scaler = StandardScaler()
X['Amount'] = scaler.fit_transform(X['Amount'].values.reshape(-1, 1))

# Step 5: Handle Class Imbalance with SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X, y)
print("After SMOTE:\n", y_resampled.value_counts())

# Step 6: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 7: Initialize Models
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

results = {}

# Step 8: Training, Evaluation, and Visualization
for name, model in models.items():
    print(f"\nTraining: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # Metrics
    cm = confusion_matrix(y_test, y_pred)
    cr = classification_report(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)

    # Store results
    results[name] = {'model': model, 'cm': cm, 'report': cr, 'auc': auc, 'fpr': fpr, 'tpr': tpr}

    # Confusion Matrix Visualization
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # ROC Curve Visualization
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{name} - ROC Curve')
    plt.legend()
    plt.grid()
    plt.show()

    # Feature Importance Visualization
    if name in ["Random Forest", "XGBoost"]:
        feature_importances = pd.Series(model.feature_importances_, index=X.columns)
        feature_importances.nlargest(10).plot(kind='barh')
        plt.title(f'{name} - Top 10 Feature Importances')
        plt.xlabel('Importance')
        plt.gca().invert_yaxis()
        plt.grid()
        plt.tight_layout()
        plt.show()

    # Print classification report
    print(f"{name} - Classification Report:\n{cr}")

# Step 9: Compare Models by ROC
plt.figure(figsize=(8, 6))
for name, res in results.items():
    plt.plot(res['fpr'], res['tpr'], label=f"{name} (AUC = {res['auc']:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.grid()
plt.show()

# Step 10: Best Model Summary
best_model = max(results.items(), key=lambda x: x[1]['auc'])
print(f"\nBest Model: {best_model[0]} with AUC = {best_model[1]['auc']:.4f}")